{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LFOWF-AjZDV",
        "outputId": "3b9185b0-235e-4d1e-b81a-458c205df376"
      },
      "outputs": [],
      "source": [
        "# !pip install -q xlrd\n",
        "# !git clone https://github.com/skotak2/Seq2Seq-Machine-Translation-Model-Kannada-to-English.git\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "08z5gtHVOnVo"
      },
      "outputs": [],
      "source": [
        "#Importing the required libraries for building the nueral network\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "eCHWBcpx3dW1"
      },
      "outputs": [],
      "source": [
        "#Defining Start and end of the sentense. SOS - start of the sentence and EOS - end of the sentences\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "#Creating word to index and frequency of each words in a dictionary. As the nueral network accepts input as a vector,\n",
        "#We index every word with a number. Each unique number or vector could represent a word for input and output of text.\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "5i0_wLPa4EnD"
      },
      "outputs": [],
      "source": [
        "# Turn a Unicode string to plain ASCII\n",
        "\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "_BK1qlfb4Th0"
      },
      "outputs": [],
      "source": [
        "def readLangs(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines, \n",
        "    # the file in uploaded to the sessions cache of google colab, the file address needs to changed\n",
        "    lines = open('data.txt', encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[s for s in l.split('\\t')] for l in lines]\n",
        "    l = []\n",
        "    for p in pairs:\n",
        "      l.append(p[0:2])\n",
        "    pairs = l\n",
        "\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "LjpryYjS4w6F"
      },
      "outputs": [],
      "source": [
        "#Declaring function to format data into readable formats\n",
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1,lang2,reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0QzG5c58ZBQ",
        "outputId": "418eb1b7-5d3c-4f1b-e314-8b8e19cc17b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Read 153 sentence pairs\n",
            "Trimmed to 153 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "kan 487\n",
            "eng 496\n",
            "['ಅಪಘಾತದ ಕಾರಣವೇ ಗೊತ್ತಿಲ್ಲ.', 'The cause of the accident is unknown.']\n"
          ]
        }
      ],
      "source": [
        "#The following is a sample list, [Kannada, English]\n",
        "input_lang_kan, output_lang_eng, pairs = prepareData('eng','kan',True)\n",
        "print(random.choice(pairs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "is80BpGab9tV"
      },
      "outputs": [],
      "source": [
        "#Defining objects to create our lookup dictionaries for deployment\n",
        "input_lkp = input_lang_kan.word2index\n",
        "output_lkp = output_lang_eng.index2word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "cAzYjIDnU_zc"
      },
      "outputs": [],
      "source": [
        "#We pickle the lookup libraries for deployment.\n",
        "# picklefile = open('/content/drive/My Drive/Kan_Eng/input_kan', 'wb')\n",
        "# pickle.dump(input_lkp, picklefile)\n",
        "# picklefile.close()\n",
        "\n",
        "# picklefile = open('/content/drive/My Drive/Kan_Eng/output_eng', 'wb')\n",
        "# pickle.dump(output_lkp, picklefile)\n",
        "# picklefile.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nz4k69W81tGP",
        "outputId": "3caa274f-fd6c-4900-9ef3-f153737c37e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7\n",
            "14\n"
          ]
        }
      ],
      "source": [
        "#Checking maximum length of sentence in the given dataset\n",
        "for i in range(len(pairs)):\n",
        "  a = 0\n",
        "  b = 0\n",
        "  if len(pairs[i][0].split()) > a:\n",
        "    a = len(pairs[i][0].split())\n",
        "  if len(pairs[i][1].split())>b:\n",
        "    b = len(pairs[i][1].split())\n",
        "print(a)\n",
        "print(b)\n",
        "#Maximum length of the kannada Sentence is 7\n",
        "#maximum length of a english sentence is 14\n",
        "#Hence Declaring Max_length as 15\n",
        "MAX_LENGTH = 15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "3L7hxoWh8cE4"
      },
      "outputs": [],
      "source": [
        "#Defining the class for the Encoder, the construct of layers and the RNN for encoding a given input sentence\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "cs8BubNCF8ak"
      },
      "outputs": [],
      "source": [
        "#Decoder have the layers of nueral network and RNN as defined below, this class would work along with attention mechanism\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "-qWMpy7NGR6x"
      },
      "outputs": [],
      "source": [
        "#This class would use attention weights to each hidden state of encoder and build the context vector that would go in decoding the words in a sequence.\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "eFDrEUazHQFT"
      },
      "outputs": [],
      "source": [
        "# We define the below functions to generate tensors from the text using the indexes assigned to each word. These tensors are used to feed the network\n",
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang_kan, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang_eng, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "JAxHvxAaHrVq"
      },
      "outputs": [],
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "# We define the training function below\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "aULnERgLHx92"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "47DbrWYAH3K9"
      },
      "outputs": [],
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "29mMXY4rH7Mf"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "IIrjeFrhH7P1"
      },
      "outputs": [],
      "source": [
        "def evaluate_eng(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang_kan, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang_eng.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "            sen = ' '.join(decoded_words)\n",
        "\n",
        "        return sen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vNoEVdOIGuO",
        "outputId": "19787893-ce69-4248-e3ff-bff983696022"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0m 58s (- 13m 40s) (5000 6%) 3.2513\n",
            "2m 0s (- 13m 5s) (10000 13%) 0.3648\n",
            "3m 1s (- 12m 7s) (15000 20%) 0.0448\n",
            "4m 3s (- 11m 8s) (20000 26%) 0.0234\n",
            "5m 4s (- 10m 9s) (25000 33%) 0.0157\n",
            "6m 5s (- 9m 8s) (30000 40%) 0.0118\n",
            "7m 7s (- 8m 8s) (35000 46%) 0.0094\n",
            "8m 8s (- 7m 7s) (40000 53%) 0.0078\n",
            "9m 9s (- 6m 6s) (45000 60%) 0.0067\n",
            "10m 11s (- 5m 5s) (50000 66%) 0.0061\n",
            "11m 13s (- 4m 4s) (55000 73%) 0.0053\n",
            "12m 17s (- 3m 4s) (60000 80%) 0.0046\n",
            "13m 25s (- 2m 3s) (65000 86%) 0.0042\n",
            "14m 27s (- 1m 1s) (70000 93%) 0.0038\n",
            "15m 34s (- 0m 0s) (75000 100%) 0.0036\n"
          ]
        }
      ],
      "source": [
        "#Training the model\n",
        "hidden_size = 100\n",
        "encoder_eng = EncoderRNN(input_lang_kan.n_words, hidden_size).to(device)\n",
        "attn_decoder_eng = AttnDecoderRNN(hidden_size, output_lang_eng.n_words,dropout_p=0.1).to(device)\n",
        "\n",
        "trainIters(encoder_eng, attn_decoder_eng, 75000, print_every=5000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgxigqrIbR_3",
        "outputId": "eacdfad3-eb83-4a23-8411-4dca57cd97d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "complete\n"
          ]
        }
      ],
      "source": [
        "#Save the weights for deployment\n",
        "torch.save(encoder_eng.state_dict(), \"model_enc_eng.dict\")\n",
        "torch.save(attn_decoder_eng.state_dict(), \"model_dec_eng.dict\")\n",
        "print(\"complete\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Word Error Rate (WER) is a commonly used evaluation metric for speech recognition and speech-to-text systems. It measures the percentage of words that are incorrectly recognized or transcribed by the system. To calculate WER, we need to compare the predicted transcript with the ground truth transcript and count the number of substitutions, deletions, and insertions that are required to transform the predicted transcript into the ground truth transcript."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def wer(predicted, ground_truth):\n",
        "    \"\"\"\n",
        "    Calculates the Word Error Rate (WER) given the predicted and ground truth transcripts.\n",
        "\n",
        "    :param predicted: A list of words representing the predicted transcript.\n",
        "    :param ground_truth: A list of words representing the ground truth transcript.\n",
        "    :return: The Word Error Rate (WER) as a float.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize the matrices.\n",
        "    D = [[0] * (len(ground_truth) + 1) for _ in range(len(predicted) + 1)]\n",
        "    for i in range(len(predicted) + 1):\n",
        "        D[i][0] = i\n",
        "    for j in range(len(ground_truth) + 1):\n",
        "        D[0][j] = j\n",
        "\n",
        "    # Compute the WER.\n",
        "    for i in range(1, len(predicted) + 1):\n",
        "        for j in range(1, len(ground_truth) + 1):\n",
        "            if predicted[i - 1] == ground_truth[j - 1]:\n",
        "                D[i][j] = D[i - 1][j - 1]\n",
        "            else:\n",
        "                substitution = D[i - 1][j - 1] + 1\n",
        "                insertion = D[i][j - 1] + 1\n",
        "                deletion = D[i - 1][j] + 1\n",
        "                D[i][j] = min(substitution, insertion, deletion)\n",
        "\n",
        "    # Return the WER.\n",
        "    return D[-1][-1] / len(ground_truth)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Bilingual Evaluation Understudy (BLEU) score is a commonly used evaluation metric for machine translation systems, including speech-to-speech translation. It measures the degree of overlap between the predicted translation and the ground truth translation based on n-gram matches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "def bleu(predicted, ground_truth):\n",
        "    \"\"\"\n",
        "    Calculates the BLEU score given the predicted and ground truth translations.\n",
        "\n",
        "    :param predicted: A list of words representing the predicted translation.\n",
        "    :param ground_truth: A list of words representing the ground truth translation.\n",
        "    :return: The BLEU score as a float.\n",
        "    \"\"\"\n",
        "\n",
        "    # Compute the n-gram overlap.\n",
        "    weights = [(1.0/4) for _ in range(4)]\n",
        "    n_grams = [nltk.translate.bleu_score.sentence_bleu([ground_truth], predicted, weights=weights)]\n",
        "    \n",
        "    # Return the BLEU score.\n",
        "    return sum(n_grams) / len(n_grams)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Kannada_to_English_Machine_Translation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
